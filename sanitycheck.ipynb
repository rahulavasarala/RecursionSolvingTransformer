{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f1eccb-9122-4043-8930-1cd014dd7b4f",
   "metadata": {},
   "source": [
    "### Sanity Checks for each individual component in transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443561c2-1599-4cc9-aae5-d6ab82ff43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "from transformer import LayerNorm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b2609-9932-44c2-a243-24fff6e7e49d",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cea5b5-92e2-408b-be48-bfdd5aee94c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Scaled Dot Product PASSED!\n",
      "TEST: Scaled Dot Product PASSED!\n"
     ]
    }
   ],
   "source": [
    "from transformer import scaled_dot_product_attention\n",
    "\n",
    "def test_scaled_dot_product(q, k, v):\n",
    "    values, attention = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "    #assuming that q k and v are 2d matrices\n",
    "    similarity = (q @ k.T)/math.sqrt(q.size()[-1])\n",
    "    similarity = F.softmax(similarity, dim = -1)\n",
    "\n",
    "    expected_values = similarity @ v\n",
    "    expected_attention = similarity\n",
    "\n",
    "    # print(values)\n",
    "    # print(expected_values)\n",
    "\n",
    "    # print(attention)\n",
    "    # print(expected_attention)\n",
    "\n",
    "    assert torch.allclose(values, expected_values) and torch.allclose(attention, expected_attention)\n",
    "    print(\"TEST: Scaled Dot Product PASSED!\")\n",
    "    \n",
    "\n",
    "test_scaled_dot_product(torch.ones((2,2)), torch.ones((2,2)), torch.ones((2,2)))\n",
    "test_scaled_dot_product(torch.ones((2,1)), torch.ones((2, 1)), torch.ones((2,1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ee6d3-5c67-4b60-934a-85772140d986",
   "metadata": {},
   "source": [
    "#### Multiheaded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc954ff-6fcc-47ba-85a6-3e667c8796a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Multiheaded Attention PASSED!\n"
     ]
    }
   ],
   "source": [
    "#In order to test this out, let us assume that the batch dimension = 1, the max sequence length is 2, and d model is 6\n",
    "#and also let us try to make the Wq, Wk, and Wv matrices the identity\n",
    "#The weight matrix actually\n",
    "\n",
    "from transformer import MultiheadedAttention\n",
    "\n",
    "def test_multihead_attention():\n",
    "    q = torch.ones((1,2,4))\n",
    "    mha = MultiheadedAttention(num_heads = 2, d_model = 4, identity = True)\n",
    "    output = mha(q)\n",
    "    assert torch.allclose(q, output)\n",
    "    print(\"TEST: Multiheaded Attention PASSED!\")\n",
    "\n",
    "test_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee874d-f87a-4edc-bae4-cc6a5df3d6f2",
   "metadata": {},
   "source": [
    "#### Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1277fc-d1cf-4245-a8b2-37592c865fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: MH Cross Attention PASSED!\n"
     ]
    }
   ],
   "source": [
    "from transformer import MultiheadedCrossAttention\n",
    "\n",
    "def test_cross_attention():\n",
    "    x = torch.ones((1,2,4))\n",
    "    y = torch.ones((1,2,4))\n",
    "    mca = MultiheadedCrossAttention(num_heads = 2, d_model = 4, identity = True)\n",
    "    output = mca(x, y)\n",
    "    assert torch.allclose(x, output)\n",
    "    print(\"TEST: MH Cross Attention PASSED!\")\n",
    "\n",
    "test_cross_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479979e-c71e-42a2-9f91-084b4abd8a97",
   "metadata": {},
   "source": [
    "#### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1489156b-5800-49cb-9c19-79f2da2f2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "TEST: Encoder Throughput PASSED!\n",
      "TEST: Encoder Chain Throughput PASSED!\n"
     ]
    }
   ],
   "source": [
    "from transformer import Encoder\n",
    "from transformer import EncoderChain\n",
    "\n",
    "##Create random input which matches batch size, max_sequence_length, and d_model\n",
    "\n",
    "def test_encoder_output():\n",
    "\n",
    "    batch_size = 3\n",
    "    d_model = 6\n",
    "    max_sequence_length = 10\n",
    "    num_heads = 2\n",
    "\n",
    "    x = torch.rand((batch_size, max_sequence_length, d_model))\n",
    "    print(x.dtype)\n",
    "    mask = torch.rand((max_sequence_length, max_sequence_length))\n",
    "\n",
    "    enc = Encoder(d_model = d_model, num_heads = num_heads, ffn_hidden = 10, drop_prob = 0.1)\n",
    "    output = enc(x,mask)\n",
    "\n",
    "    assert output.size() == x.size()\n",
    "    print(\"TEST: Encoder Throughput PASSED!\")\n",
    "\n",
    "def test_encoder_chain_output():\n",
    "\n",
    "    batch_size = 3\n",
    "    d_model = 6\n",
    "    max_sequence_length = 10\n",
    "    num_heads = 2\n",
    "    chain_length = 3\n",
    "\n",
    "    x = torch.rand((batch_size, max_sequence_length, d_model))\n",
    "    mask = torch.rand((max_sequence_length, max_sequence_length))\n",
    "\n",
    "    enc = Encoder(d_model = d_model, num_heads = num_heads, ffn_hidden = 10, drop_prob = 0.1)\n",
    "    enc_chain = EncoderChain(\n",
    "                    *[Encoder(\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        ffn_hidden=10,\n",
    "                        drop_prob=0.1\n",
    "                    ) for _ in range(chain_length)]\n",
    "                )\n",
    "    output = enc_chain(x,mask)\n",
    "    output_single = enc(x,mask)\n",
    "\n",
    "    assert output.size() == x.size()\n",
    "    assert torch.allclose(output, output_single) == False\n",
    "    print(\"TEST: Encoder Chain Throughput PASSED!\")\n",
    "\n",
    "test_encoder_output()\n",
    "test_encoder_chain_output()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04dcfd-6f81-4b5f-b5d3-007e97522477",
   "metadata": {},
   "source": [
    "#### DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f284ea-e7a0-414b-a217-ce91396dd4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decoder Throughput PASSED!\n",
      "TEST: Decoder Chain Throughput PASSED!\n"
     ]
    }
   ],
   "source": [
    "from transformer import Decoder\n",
    "from transformer import DecoderChain\n",
    "\n",
    "##Create random input which matches batch size, max_sequence_length, and d_model\n",
    "\n",
    "def test_decoder_output():\n",
    "\n",
    "    batch_size = 3\n",
    "    d_model = 6\n",
    "    max_sequence_length = 10\n",
    "    num_heads = 2\n",
    "\n",
    "    x = torch.rand((batch_size, max_sequence_length, d_model))\n",
    "    mask = torch.rand((max_sequence_length, max_sequence_length))\n",
    "\n",
    "    dec = Decoder(d_model = d_model, num_heads = num_heads, ffn_hidden = 10, drop_prob = 0.1)\n",
    "    output = dec(x,x, mask, mask)\n",
    "\n",
    "    assert output.size() == x.size()\n",
    "    print(\"TEST: Decoder Throughput PASSED!\")\n",
    "\n",
    "def test_decoder_chain_output():\n",
    "\n",
    "    batch_size = 3\n",
    "    d_model = 6\n",
    "    max_sequence_length = 10\n",
    "    num_heads = 2\n",
    "    chain_length = 3\n",
    "\n",
    "    x = torch.rand((batch_size, max_sequence_length, d_model))\n",
    "    mask = torch.rand((max_sequence_length, max_sequence_length))\n",
    "\n",
    "    dec = Decoder(d_model = d_model, num_heads = num_heads, ffn_hidden = 10, drop_prob = 0.1)\n",
    "    dec_chain = DecoderChain(\n",
    "                    *[Decoder(\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        ffn_hidden=10,\n",
    "                        drop_prob=0.1\n",
    "                    ) for _ in range(chain_length)]\n",
    "                )\n",
    "    output = dec_chain(x,x,mask,mask)\n",
    "    output_single = dec(x,x, mask,mask)\n",
    "\n",
    "    assert output.size() == x.size()\n",
    "    assert torch.allclose(output, output_single) == False\n",
    "    print(\"TEST: Decoder Chain Throughput PASSED!\")\n",
    "\n",
    "test_decoder_output()\n",
    "test_decoder_chain_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d695c-ec7a-41e6-8e17-e67d294aff05",
   "metadata": {},
   "source": [
    "#### LAYER NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69e1f01-16a0-4b23-be8f-5d6ce181d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Layer Norm Generation PASSED!\n"
     ]
    }
   ],
   "source": [
    "def test_layer_norm():\n",
    "    ones = torch.ones((4,4))\n",
    "    x = torch.triu(ones)\n",
    "    # print(x)\n",
    "    \n",
    "    normalizer = LayerNorm(d_model = 4, eps = 1e-5)\n",
    "    layernorm = normalizer(x)\n",
    "    # print(layernorm)\n",
    "    \n",
    "    torch_normalizer = nn.LayerNorm(normalized_shape = 4, eps = 1e-05)\n",
    "    expected = torch_normalizer(x)\n",
    "    # print(expected)\n",
    "    \n",
    "    assert torch.allclose(layernorm, expected)\n",
    "    print(\"TEST: Layer Norm Generation PASSED!\")\n",
    "\n",
    "test_layer_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b31df0-bcb8-4857-a14d-ce4c96e51958",
   "metadata": {},
   "source": [
    "#### POSITIONAL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a40746-22ad-414a-ba12-d606c6e2745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: PE Encodings PASSED!\n"
     ]
    }
   ],
   "source": [
    "from transformer import PositionalEncoder\n",
    "\n",
    "def test_positional_encoder():\n",
    "\n",
    "    pe = PositionalEncoder(max_sequence_length = 10, d_model = 6)\n",
    "\n",
    "    pos_encoding = pe.generate()\n",
    "    # print(pos_encoding)\n",
    "\n",
    "    assert 1 == 1\n",
    "    print(\"TEST: PE Encodings PASSED!\")\n",
    "\n",
    "test_positional_encoder()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea089e-e84f-4645-9eb0-beaf52344921",
   "metadata": {},
   "source": [
    "#### DATASET CREATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4180572d-997f-46c1-878f-97f5b2fff8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "[('a_n4', 'a_n4', 'a_n4'), ('an =', 'an =', 'an =')]\n",
      "[('a_n4', 'a_n4', 'a_n4'), ('an =', 'an =', 'an =')]\n",
      "[('a_n4', 'a_n4', 'a_n4'), ('an =', 'an =', 'an =')]\n",
      "[('a_n4',), ('an =',)]\n",
      "TEST: Dataset Loader PASSED!\n"
     ]
    }
   ],
   "source": [
    "from dataset import RecursionDatasetCreator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def test_dataset_creator():\n",
    "\n",
    "    START_TOKEN = '<sos>'\n",
    "    PADDING_TOKEN = '<pad>'\n",
    "    END_TOKEN = '<eos>'\n",
    "\n",
    "    recursion_vocabulary = ['a', '_', 'n', '+', '1', '=', '*', '/', '-', '^',\n",
    "                            '2', '3', '4', '5', '6', '7', '8', '0', '9', '(', ')',' ', START_TOKEN, PADDING_TOKEN, END_TOKEN]\n",
    "    \n",
    "    solution_vocabulary = ['a', '_', 'n', '+', '1', '=', '*', '/', '-', '^',\n",
    "                            '2', '3', '4', '5', '6', '7', '8', '9', '0', '(', ')',' ',START_TOKEN, PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "    dataset_creator = RecursionDatasetCreator(\"./recursions.txt\", \"./solutions.txt\", 100, 10, recursion_vocabulary, solution_vocabulary)\n",
    "\n",
    "    recursion_dataset = dataset_creator.create_recursion_dataset()\n",
    "\n",
    "    batch_size = 3\n",
    "    train_loader = DataLoader(recursion_dataset, batch_size)\n",
    "    iterator = iter(train_loader)\n",
    "\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "\n",
    "        print(batch)\n",
    "\n",
    "    \n",
    "    print(\"TEST: Dataset Loader PASSED!\")\n",
    "    return\n",
    "\n",
    "test_dataset_creator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107698f-98aa-4ee3-ab1a-0da76eed3d6d",
   "metadata": {},
   "source": [
    "#### Tokenizer and Mask Geneneration Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9f476f-7dd1-4247-b4d0-d8d6a99eee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[22,  0,  2, 24, 23],\n",
      "        [22, 10, 15, 24, 23],\n",
      "        [22, 10, 24, 23, 23]])\n",
      "torch.Size([3, 5])\n",
      "tensor([[22,  0, 24, 23, 23],\n",
      "        [22, 15, 24, 23, 23],\n",
      "        [22, 10,  4, 24, 23]])\n",
      "tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[-0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "TEST Passed! ENCODER IS GOOD, DECODER is GOOD, CROSS is GOOD!\n"
     ]
    }
   ],
   "source": [
    "from transformer import Tokenizer\n",
    "from transformer import generate_masks_tokenized\n",
    "\n",
    "def test_tokenizer():\n",
    "\n",
    "    START = '<sos>'\n",
    "    PAD = '<pad>'\n",
    "    END = '<eos>'\n",
    "\n",
    "    recursion_vocabulary = ['a', '_', 'n', '+', '1', '=', '*', '/', '-', '^',\n",
    "                            '2', '3', '4', '5', '6', '7', '8', '0', '9', '(', ')',' ', START, PAD, END]\n",
    "    \n",
    "    solution_vocabulary = ['a', '_', 'n', '+', '1', '=', '*', '/', '-', '^',\n",
    "                            '2', '3', '4', '5', '6', '7', '8', '9', '0', '(', ')',' ',START, PAD, END]\n",
    "\n",
    "    in_tkzr = Tokenizer(recursion_vocabulary, START, PAD, END)\n",
    "\n",
    "    in_sentences = [\"an\", \"27\", \"2\"]\n",
    "    tok_input = in_tkzr.tokenize(in_sentences)\n",
    "    tok_input = in_tkzr.pad(tok_input, 5, start = True, end = True)\n",
    "    print(tok_input)\n",
    "\n",
    "    out_sentences = [\"a\", \"7\", \"21\"]\n",
    "    out_tkzr = Tokenizer(solution_vocabulary, START, PAD, END)\n",
    "    tok_output = out_tkzr.tokenize(out_sentences)\n",
    "    tok_output = out_tkzr.pad(tok_output, 5, start = True, end = True)\n",
    "    print(tok_output)\n",
    "\n",
    "    enc_mask, dec_mask, cross_mask = generate_masks_tokenized(tok_input, tok_output, 23, 23)\n",
    "    print(enc_mask)\n",
    "    print(dec_mask)\n",
    "    print(cross_mask)\n",
    "    print(\"TEST Passed! ENCODER IS GOOD, DECODER is GOOD, CROSS is GOOD!\")\n",
    "\n",
    "test_tokenizer()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c236ada-368c-4f3a-8e06-1fd99b7e0ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1111 ^111)*11^(1+ -1) ((1111 ^ 111) * (11 ^ (1 + -1)))\n",
      "1*(1 + 1) + 1 * 1 + + 1 1 1\n",
      "1*(1 + 1) + 1 (1 * ((1 + 1) + 1))\n"
     ]
    }
   ],
   "source": [
    "from serialization import serialize\n",
    "from serialization import unserialize\n",
    "\n",
    "def test_serialize_unserialize(expression):\n",
    "\n",
    "    print(expression, unserialize(serialize(expression)))\n",
    "    return\n",
    "\n",
    "          \n",
    "test_serialize_unserialize(\"(1111 ^111)*11^(1+ -1)\")\n",
    "\n",
    "def test_serialize_two(expression):\n",
    "\n",
    "    print(expression, serialize(expression))\n",
    "    print(expression, unserialize(serialize(expression)))\n",
    "\n",
    "test_serialize_two(\"1*(1 + 1) + 1\")\n",
    "\n",
    "#ok so there is this flaw in the serialize and unserialize algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefc08e-0228-4b63-8b86-a4cdc77b840a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
